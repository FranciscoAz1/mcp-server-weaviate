{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeoutConfig not available in this weaviate version; using default client timeouts.\n",
      "Server modules detected: ['generative-anthropic', 'generative-anyscale', 'generative-aws', 'generative-cohere', 'generative-databricks', 'generative-friendliai', 'generative-google', 'generative-mistral', 'generative-nvidia', 'generative-octoai', 'generative-ollama', 'generative-openai', 'generative-xai', 'multi2multivec-jinaai', 'multi2vec-cohere', 'multi2vec-google', 'multi2vec-jinaai', 'multi2vec-nvidia', 'multi2vec-voyageai', 'reranker-cohere', 'reranker-jinaai', 'reranker-nvidia', 'reranker-voyageai', 'text2multivec-jinaai', 'text2vec-aws', 'text2vec-cohere', 'text2vec-databricks', 'text2vec-google', 'text2vec-huggingface', 'text2vec-jinaai', 'text2vec-mistral', 'text2vec-nvidia', 'text2vec-octoai', 'text2vec-openai', 'text2vec-transformers', 'text2vec-voyageai', 'text2vec-weaviate']\n",
      "Collection 'Dataset' ready (text2vec-transformers).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Francisco Azeredo\\.conda\\envs\\tese\\Lib\\site-packages\\weaviate\\warnings.py:196: DeprecationWarning: Dep024: You are using the `vectorizer_config` argument in `collection.config.create()`, which is deprecated.\n",
      "            Use the `vector_config` argument instead.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: RAG Initialization (Run First)\n",
    "# -------------------------------------\n",
    "# Sets up Weaviate collection using text2vec-transformers (external inference container).\n",
    "# Ensure docker-compose is up with services: weaviate + t2v-transformers.\n",
    "#   ENABLE_MODULES=text2vec-transformers,generative-ollama\n",
    "#   DEFAULT_VECTORIZER_MODULE=text2vec-transformers\n",
    "#   TRANSFORMERS_INFERENCE_API=http://t2v-transformers:8080\n",
    "# Optional: generative-ollama (Ollama running on host for qwen2m:latest)\n",
    "# Timeout tuning: Some weaviate client versions expose TimeoutConfig; if not, we fall back.\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "\n",
    "# Attempt optional TimeoutConfig (newer weaviate client). If missing, continue with defaults.\n",
    "try:\n",
    "    from weaviate.connect import TimeoutConfig  # may not exist in older versions\n",
    "    TIMEOUTS = TimeoutConfig(init=60, query=180, insert=120)\n",
    "    client = weaviate.connect_to_local(timeout_config=TIMEOUTS)\n",
    "    print(\"Custom timeouts applied:\", TIMEOUTS)\n",
    "except ImportError:\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig not available in this weaviate version; using default client timeouts.\")\n",
    "except TypeError:\n",
    "    # Signature mismatch (older version). Reconnect without custom timeouts.\n",
    "    client = weaviate.connect_to_local()\n",
    "    TIMEOUTS = None\n",
    "    print(\"TimeoutConfig signature unsupported; using default timeouts.\")\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"qwen2.5:latest\"  # must match the model available to Ollama on host\n",
    "\n",
    "# Diagnostics\n",
    "try:\n",
    "    meta = client.get_meta()\n",
    "    print(\"Server modules detected:\", list(meta.get(\"modules\", {}).keys()))\n",
    "except Exception as e:\n",
    "    print(\"Meta fetch failed:\", e)\n",
    "\n",
    "# Recreate collection for a clean slate\n",
    "try:\n",
    "    client.collections.delete(\"Dataset\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "api_endpoint = \"http://host.docker.internal:11434\"  # Ollama on host\n",
    "\n",
    "client.collections.create(\n",
    "    \"Dataset\",\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT, tokenization=Tokenization.LOWERCASE),\n",
    "        Property(name=\"file_path\", data_type=DataType.TEXT)\n",
    "    ],\n",
    "    vectorizer_config=[\n",
    "        Configure.NamedVectors.text2vec_transformers(\n",
    "            name=\"text_vector\",\n",
    "            source_properties=[\"text\"],\n",
    "            pooling_strategy=\"masked_mean\",\n",
    "        )\n",
    "    ],\n",
    "    generative_config=Configure.Generative.ollama(\n",
    "        api_endpoint=api_endpoint,\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "assert client.collections.exists(\"Dataset\")\n",
    "print(\"Collection 'Dataset' ready (text2vec-transformers).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca46258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading documents...\n",
      "Loaded 442 docs in 3.46s\n",
      "Start RSS: 531.56 MB\n",
      "Indexing (batched)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7028c8f1943b4318bfe984a4ef445403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 442 / 442 docs in 6.45s (68.55 docs/s)\n",
      "End RSS: 537.34 MB (Δ 5.78 MB)\n",
      "Indexing complete. Proceed to Cell 2 for querying & evaluation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json, random, time, gc\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from tqdm.auto import tqdm\n",
    "# ---------------- User Config ----------------\n",
    "SHUFFLE_DOCS = True\n",
    "MAX_DOCS = None  # set int to limit docs\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DATASET_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\dataset\\\\LiHua-World\\\\data\\\\\"\n",
    "WORKING_DIR = r\"C:\\\\Users\\\\Francisco Azeredo\\\\OneDrive\\\\Documents\\\\tecnico\\\\5 ano\\\\tese\\\\Código\\\\MiniRAG\\\\notebooks\\\\storage\"\n",
    "LLM_MODEL_NAME = \"qwen2m:latest\"\n",
    "LOG_LEVEL = \"CRITICAL\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "try:\n",
    "    PROCESS = psutil.Process()\n",
    "except Exception:\n",
    "    PROCESS = None\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def memory_mb():\n",
    "    if PROCESS is None: return None\n",
    "    return PROCESS.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "def read_text_from_file(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    try:\n",
    "        if suffix in {\".txt\", \".md\"}:\n",
    "            return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if suffix == \".json\":\n",
    "            data = json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                if isinstance(data, dict) and k in data and isinstance(data[k], str):\n",
    "                    return data[k]\n",
    "            return json.dumps(data)\n",
    "        if suffix in {\".jsonl\", \".ndjson\"}:\n",
    "            lines = []\n",
    "            with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line=line.strip()\n",
    "                    if not line: continue\n",
    "                    try:\n",
    "                        obj=json.loads(line)\n",
    "                        if isinstance(obj, dict):\n",
    "                            for k in (\"text\",\"content\",\"body\",\"article\"):\n",
    "                                if k in obj and isinstance(obj[k], str):\n",
    "                                    lines.append(obj[k]); break\n",
    "                            else:\n",
    "                                lines.append(json.dumps(obj))\n",
    "                        else:\n",
    "                            lines.append(str(obj))\n",
    "                    except Exception:\n",
    "                        lines.append(line)\n",
    "            return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_READING_FILE {path.name}: {e}\"\n",
    "    return \"\"\n",
    "\n",
    "def load_documents(root_dir: str):\n",
    "    exts = (\".txt\", \".md\", \".json\", \".jsonl\", \".ndjson\")\n",
    "    paths = [p for p in Path(root_dir).rglob(\"*\") if p.suffix.lower() in exts and p.is_file()]\n",
    "    if SHUFFLE_DOCS: random.shuffle(paths)\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        if MAX_DOCS and len(docs) >= MAX_DOCS: break\n",
    "        text = read_text_from_file(p).strip()\n",
    "        if not text: continue\n",
    "        docs.append({\"id\": f\"doc_{len(docs)}\", \"text\": text, \"source_path\": str(p)})\n",
    "    return docs\n",
    "\n",
    "# ---------------- Indexing ----------------\n",
    "async def index_documents(rag):\n",
    "    print(\"Loading documents...\")\n",
    "    t0 = time.perf_counter(); docs = load_documents(DATASET_DIR)\n",
    "    print(f\"Loaded {len(docs)} docs in {time.perf_counter()-t0:.2f}s\")\n",
    "    if not docs:\n",
    "        print(\"No documents found; adjust DATASET_DIR.\"); return\n",
    "    start_mem = memory_mb()\n",
    "    if start_mem is not None: print(f\"Start RSS: {start_mem:.2f} MB\")\n",
    "    texts = [d['text'] for d in docs]\n",
    "    metas = [{\"id\": d['id'], \"source\": d['source_path']} for d in docs]\n",
    "    print(\"Indexing (batched)...\")\n",
    "    t1 = time.perf_counter()\n",
    "    failed = 0\n",
    "    with rag.batch.dynamic() as batch:\n",
    "        for text, metadata in tqdm(zip(texts, metas), desc=\"Indexing\", total=len(texts)):\n",
    "            try:\n",
    "                batch.add_object(\n",
    "                    properties={\n",
    "                        \"text\": text,\n",
    "                        \"file_path\": metadata.get(\"source\")\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                if failed < 5:\n",
    "                    print(f\"Failed {metadata.get('id')}: {e}\")\n",
    "    dur = time.perf_counter()-t1\n",
    "    print(f\"Inserted {len(texts)-failed} / {len(texts)} docs in {dur:.2f}s ({((len(texts)-failed)/dur) if dur>0 else 0:.2f} docs/s)\")\n",
    "    if failed:\n",
    "        print(f\"Total failed: {failed}\")\n",
    "    gc.collect(); end_mem = memory_mb()\n",
    "    if end_mem is not None: print(f\"End RSS: {end_mem:.2f} MB (Δ {end_mem - start_mem:.2f} MB)\")\n",
    "\n",
    "rag = client.collections.get(\"Dataset\")\n",
    "await index_documents(rag)\n",
    "print(\"Indexing complete. Proceed to Cell 2 for querying & evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 637 QA pairs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e12c34db314a518ca74029ffe9c1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval-light:   0%|          | 0/637 [00:00<?, ?q/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Did Adam Smith send a message to Li Hua about the upcoming building maintenance ...\n",
      "Answer: [timeout after 75.0s attempts=2 last_error=Query call with protocol GRPC search failed with message Deadline Exceeded.]\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '-0.028'} Latency: 75015.9 ms attempts=2\n",
      "-\n",
      "Q2: Did Wolfgang ask Li Hua about watching \"Star Wars: A New Hope\" after he asked Li...\n",
      "Answer: No, Wolfgang did not ask Li Hua about watching \"Star Wars: A New Hope\" after asking about going to see \"Overwatch 3.\" The conversation in the provided text only\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '0.097'} Latency: 17600.8 ms attempts=1\n",
      "-\n",
      "Q2: Did Wolfgang ask Li Hua about watching \"Star Wars: A New Hope\" after he asked Li...\n",
      "Answer: No, Wolfgang did not ask Li Hua about watching \"Star Wars: A New Hope\" after asking about going to see \"Overwatch 3.\" The conversation in the provided text only\n",
      "Gold: Yes\n",
      "Metrics: {'exact': '0.000', 'substring': '0.000', 'token_recall': '0.000', 'jaccard': '0.000', 'levenshtein': '0.000', 'rouge1_f': '0.000', 'rouge2_f': '0.000', 'overlap': '0.000', 'bleu': '0.000', 'bert_cos': '0.097'} Latency: 17600.8 ms attempts=1\n",
      "-\n",
      "\n",
      "Aggregate: exact=0.16% substring=15.07% token_recall=39.63%\n",
      "  jaccard: 0.073\n",
      "  levenshtein: 0.073\n",
      "  rouge1_f: 0.125\n",
      "  rouge2_f: 0.034\n",
      "  overlap: 0.276\n",
      "  bleu: 0.025\n",
      "  bert_cos: 0.333\n",
      "Latency: avg=4795.4 ms p95=8749.1 ms\n",
      "Saved results to C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\\results_light7.csv\n",
      "Evaluation complete.\n",
      "\n",
      "Aggregate: exact=0.16% substring=15.07% token_recall=39.63%\n",
      "  jaccard: 0.073\n",
      "  levenshtein: 0.073\n",
      "  rouge1_f: 0.125\n",
      "  rouge2_f: 0.034\n",
      "  overlap: 0.276\n",
      "  bleu: 0.025\n",
      "  bert_cos: 0.333\n",
      "Latency: avg=4795.4 ms p95=8749.1 ms\n",
      "Saved results to C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\\results_light7.csv\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Query & QA Evaluation\n",
    "# ----------------------------------------------\n",
    "# Run AFTER Cell 1. Uses the global `rag` object and indexed data.\n",
    "# Timeout mitigation strategies applied: reduced limit, concise prompts, retries with backoff.\n",
    "\n",
    "import os, csv, time, json, random, re, statistics, asyncio, math\n",
    "from pathlib import Path\n",
    "from minirag import QueryParam\n",
    "from minirag.utils import calculate_similarity  # legacy helper (returns indices) – not used now\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- Configuration --------\n",
    "QA_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\dataset\\LiHua-World\\qa\\query_set.csv\"\n",
    "OUTPUT_CSV_PATH = r\"C:\\Users\\Francisco Azeredo\\OneDrive\\Documents\\tecnico\\5 ano\\tese\\Código\\MiniRAG\\notebooks\"  # set to None to skip saving\n",
    "TOP_K = 4            # lower to reduce vector fetch time\n",
    "MAX_Q = None         # limit question count\n",
    "RANDOM_SEED = 42\n",
    "USE_BERT_SIM = True  # semantic metrics cost\n",
    "PER_QUERY_DEADLINE = 55.0  # seconds, must be < client.query timeout\n",
    "MAX_RETRIES = 2\n",
    "RETRY_BACKOFF = 5.0  # seconds added each retry\n",
    "PROMPT_PREFIX = \"Answer briefly: \"  # keep prompt short -> faster generation\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "TOKEN_SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)\n",
    "_ROUGE = None\n",
    "_BERT_MODEL = None\n",
    "_SMOOTH = SmoothingFunction().method1\n",
    "\n",
    "def _lazy_rouge():\n",
    "    global _ROUGE\n",
    "    if _ROUGE is None:\n",
    "        _ROUGE = Rouge()\n",
    "    return _ROUGE\n",
    "\n",
    "def _lazy_bert():\n",
    "    global _BERT_MODEL\n",
    "    if _BERT_MODEL is None:\n",
    "        _BERT_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _BERT_MODEL\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return TOKEN_SPLIT_RE.sub(\" \", s.lower()).strip()\n",
    "\n",
    "def token_set(s: str) -> set[str]:\n",
    "    return {t for t in normalize_text(s).split() if t}\n",
    "\n",
    "def calculate_best_similarity(sentences: list[str], target: str, method=\"levenshtein\", n=1):\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    tgt_tokens = target.lower().split()\n",
    "    scores = []\n",
    "    if method == \"jaccard\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_tokens = set(s.lower().split())\n",
    "            inter = set(s_tokens) & tgt_set\n",
    "            union = set(s_tokens) | tgt_set\n",
    "            scores.append(len(inter) / len(union) if union else 0.0)\n",
    "    elif method == \"levenshtein\":\n",
    "        tgt_len = max(len(tgt_tokens), 1)\n",
    "        for s in sentences:\n",
    "            dist = edit_distance(tgt_tokens, s.lower().split())\n",
    "            norm = max(tgt_len, len(s.split()))\n",
    "            scores.append(1 - dist / norm if norm else 0.0)\n",
    "    elif method == \"rouge\":\n",
    "        key = f\"rouge-{n}\"\n",
    "        r_inst = _lazy_rouge()\n",
    "        for s in sentences:\n",
    "            r = r_inst.get_scores(s, target)\n",
    "            scores.append(r[0].get(key, {}).get(\"f\", 0.0))\n",
    "    elif method == \"bert\":\n",
    "        model = _lazy_bert()\n",
    "        embeddings = model.encode(sentences + [target], show_progress_bar=False)\n",
    "        tgt_vec = embeddings[-1]\n",
    "        tgt_norm = np.linalg.norm(tgt_vec)\n",
    "        for i in range(len(sentences)):\n",
    "            v = embeddings[i]\n",
    "            denom = (np.linalg.norm(v) * tgt_norm)\n",
    "            scores.append(float(np.dot(v, tgt_vec) / denom) if denom else 0.0)\n",
    "    elif method == \"overlap\":\n",
    "        tgt_set = set(tgt_tokens)\n",
    "        for s in sentences:\n",
    "            s_set = set(s.lower().split())\n",
    "            inter = s_set & tgt_set\n",
    "            denom = min(len(s_set), len(tgt_set))\n",
    "            scores.append(len(inter) / denom if denom else 0.0)\n",
    "    elif method == \"bleu\":\n",
    "        tgt_bleu = word_tokenize(target.lower())\n",
    "        for s in sentences:\n",
    "            s_bleu = word_tokenize(s.lower())\n",
    "            scores.append(sentence_bleu([tgt_bleu], s_bleu, smoothing_function=_SMOOTH))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method.\")\n",
    "    return max(scores) if scores else 0.0\n",
    "\n",
    "def compute_similarity(answer: str, gold: str, use_bert: bool = True) -> dict:\n",
    "    sentences = sent_tokenize(answer)\n",
    "    return {\n",
    "        'jaccard': calculate_best_similarity(sentences, gold, method=\"jaccard\"),\n",
    "        'levenshtein': calculate_best_similarity(sentences, gold, method=\"levenshtein\"),\n",
    "        'rouge1_f': calculate_best_similarity(sentences, gold, method=\"rouge\", n=1),\n",
    "        'rouge2_f': calculate_best_similarity(sentences, gold, method=\"rouge\", n=2),\n",
    "        'overlap': calculate_best_similarity(sentences, gold, method=\"overlap\"),\n",
    "        'bleu': calculate_best_similarity(sentences, gold, method=\"bleu\"),\n",
    "        'bert_cos': calculate_best_similarity(sentences, gold, method=\"bert\") if use_bert else None,\n",
    "    }\n",
    "\n",
    "def compute_metrics(answer: str, gold: str, use_bert: bool = True) -> dict:\n",
    "    a_norm, g_norm = normalize_text(answer), normalize_text(gold)\n",
    "    exact = bool(g_norm) and a_norm == g_norm\n",
    "    substring = bool(g_norm) and g_norm in a_norm\n",
    "    ts_a, ts_g = token_set(answer), token_set(gold)\n",
    "    token_recall = (len(ts_a & ts_g) / len(ts_g)) if ts_g else 0.0\n",
    "    sim = compute_similarity(answer, gold, use_bert=use_bert)\n",
    "    if sim.get('bert_cos') is None:\n",
    "        sim.pop('bert_cos', None)\n",
    "    return {'exact': exact, 'substring': substring, 'token_recall': token_recall, **sim}\n",
    "\n",
    "# -------- Load QA Pairs --------\n",
    "qa_pairs = []\n",
    "if os.path.exists(QA_CSV_PATH):\n",
    "    with open(QA_CSV_PATH, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if \"Question\" in row and \"Gold Answer\" in row:\n",
    "                qa_pairs.append((row[\"Question\"].strip(), row[\"Gold Answer\"].strip()))\n",
    "else:\n",
    "    print(\"QA CSV not found. Provide QA_CSV_PATH or create synthetic pairs manually.\")\n",
    "if MAX_Q:\n",
    "    qa_pairs = qa_pairs[:MAX_Q]\n",
    "print(f\"Loaded {len(qa_pairs)} QA pairs.\")\n",
    "if not qa_pairs:\n",
    "    raise SystemExit(\"No QA data available.\")\n",
    "\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from weaviate.exceptions import WeaviateQueryError\n",
    "\n",
    "async def run_eval(mode, n):\n",
    "    rows, latencies = [], []\n",
    "    for i, (question, gold) in enumerate(tqdm(qa_pairs, total=len(qa_pairs), desc=f\"Eval-{mode}\", unit=\"q\"), start=1):\n",
    "        attempt = 0\n",
    "        answer = None\n",
    "        start_overall = time.perf_counter()\n",
    "        last_error = None\n",
    "        while attempt <= MAX_RETRIES and (time.perf_counter() - start_overall) < PER_QUERY_DEADLINE and not answer:\n",
    "            attempt += 1\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                # output = rag.generate.near_text(\n",
    "                #     query=question,\n",
    "                #     limit=TOP_K,\n",
    "                #     target_vector=\"text_vector\",\n",
    "                #     grouped_task=PROMPT_PREFIX + question[:512],  # concise prompt\n",
    "                #     return_metadata=MetadataQuery(distance=True)\n",
    "                # )\n",
    "                \n",
    "                output = rag.generate.bm25(\n",
    "                    query=question,\n",
    "                    limit=TOP_K,\n",
    "                    grouped_task=PROMPT_PREFIX + question,  # concise prompt\n",
    "                    grouped_properties=[\"text\"],\n",
    "                    return_metadata=MetadataQuery(distance=True)\n",
    "                )\n",
    "                gen = getattr(output, 'generative', None)\n",
    "                if isinstance(gen, dict):\n",
    "                    answer = gen.get('groupedResult') or gen.get('singleResult')\n",
    "                else:\n",
    "                    answer = getattr(gen, 'text', None)\n",
    "                if not answer:\n",
    "                    # fallback: concatenate retrieved texts\n",
    "                    retrieved = []\n",
    "                    for obj in getattr(output, 'objects', [])[:2]:\n",
    "                        try:\n",
    "                            retrieved.append(obj.properties.get('text', '')[:600])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    answer = (\" \\n---\\n\".join(retrieved) or \"[no answer]\")\n",
    "            except WeaviateQueryError as e:\n",
    "                last_error = e\n",
    "                if attempt <= MAX_RETRIES:\n",
    "                    time.sleep(RETRY_BACKOFF * attempt)\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                if attempt <= MAX_RETRIES:\n",
    "                    time.sleep(RETRY_BACKOFF * attempt)\n",
    "            finally:\n",
    "                latency = time.perf_counter() - t0\n",
    "        total_elapsed = time.perf_counter() - start_overall\n",
    "        if not answer:\n",
    "            answer = f\"[timeout after {total_elapsed:.1f}s attempts={attempt} last_error={last_error}]\"\n",
    "        latencies.append(total_elapsed)\n",
    "        m = compute_metrics(answer, gold)\n",
    "        rows.append({\"question\": question, \"gold\": gold, \"answer\": answer, \"latency_s\": total_elapsed, **m})\n",
    "        if i <= 2:\n",
    "            tqdm.write(f\"Q{i}: {question[:80]}...\")\n",
    "            tqdm.write(\"Answer: \" + answer[:160].replace('\\n',' '))\n",
    "            tqdm.write(\"Gold: \" + gold[:160])\n",
    "            fmt = {k: (f\"{v:.3f}\" if isinstance(v,(int,float)) and not (isinstance(v,float) and math.isnan(v)) else v) for k,v in m.items()}\n",
    "            tqdm.write(f\"Metrics: {fmt} Latency: {total_elapsed*1000:.1f} ms attempts={attempt}\")\n",
    "            tqdm.write('-')\n",
    "    def _avg(key):\n",
    "        vals = [r[key] for r in rows if key in r and isinstance(r[key], (int,float))]\n",
    "        return sum(vals)/len(vals) if vals else 0.0\n",
    "    print(f\"\\nAggregate: exact={_avg('exact'):.2%} substring={_avg('substring'):.2%} token_recall={_avg('token_recall'):.2%}\")\n",
    "    for mkey in ['jaccard','levenshtein','rouge1_f','rouge2_f','overlap','bleu','bert_cos']:\n",
    "        if mkey in rows[0]:\n",
    "            print(f\"  {mkey}: {_avg(mkey):.3f}\")\n",
    "    avg_lat = sum(latencies)/len(latencies)\n",
    "    p95_lat = sorted(latencies)[int(len(latencies)*0.95)-1] if len(latencies)>1 else latencies[0]\n",
    "    print(f\"Latency: avg={avg_lat*1000:.1f} ms p95={p95_lat*1000:.1f} ms\")\n",
    "    if OUTPUT_CSV_PATH and rows:\n",
    "        os.makedirs(OUTPUT_CSV_PATH, exist_ok=True)\n",
    "        out_file = os.path.join(OUTPUT_CSV_PATH, f\"results_{mode}{n}.csv\")\n",
    "        write_header = not os.path.exists(out_file)\n",
    "        with open(out_file,'a',encoding='utf-8',newline='') as f:\n",
    "            w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            if write_header: w.writeheader()\n",
    "            w.writerows(rows)\n",
    "        print(\"Saved results to\", out_file)\n",
    "    return rows\n",
    "\n",
    "# Run evaluation\n",
    "eval_results1 = await run_eval(\"light\", 7)\n",
    "print(\"Evaluation complete.\")\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
